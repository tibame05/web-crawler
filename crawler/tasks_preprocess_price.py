import pandas as pd
import os

from crawler.worker import app

# ğŸ¯ ä»»å‹™ï¼šæ­·å²è‚¡åƒ¹è³‡æ–™è½‰æ›
@app.task()
def read_clean_csv(path):
    # æ‰‹å‹•æŒ‡å®šæ­£ç¢ºæ¬„ä½åï¼ˆç¬¦åˆä½ çš„éœ€æ±‚ï¼‰
    correct_columns = ["Date", "Adj_Close", "Close", "High", "Low", "Open", "Volume"]

    try:
        # è·³éå‰å…©è¡Œï¼ˆPriceã€Tickerï¼‰ç›´æ¥å¾æ•¸æ“šé–‹å§‹è®€å–
        df = pd.read_csv(path, skiprows=2, header=None, names=correct_columns)

        # å°‡ Date æ¬„è½‰æˆ datetime ä¸¦è¨­ç‚º index
        df["Date"] = pd.to_datetime(df["Date"], format="%Y-%m-%d", errors="coerce")
        df = df.dropna(subset=["Date"])  # é¿å…è½‰æ›å¤±æ•—å°è‡´ NaT
        df.set_index("Date", inplace=True)

        # å¦‚æœè³‡æ–™ç‚ºç©ºæˆ– Adj_Close å…¨éƒ¨ç‚º NaNï¼Œå°±è¦–ç‚ºè½‰æ›å¤±æ•—
        if df.empty or df["Adj_Close"].isnull().all():
            return None

        return df

    except Exception as e:
        print(f"âš ï¸ è®€å–æˆ–è½‰æ›éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{path}ï¼ŒéŒ¯èª¤ï¼š{e}")
        return None
